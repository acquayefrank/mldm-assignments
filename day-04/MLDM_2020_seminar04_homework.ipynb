{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/HSE-LAMBDA/MLDM-2020/blob/master/day-04/MLDM_2020_seminar04_homework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gIxJDCLGXJ6W"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import Ridge, Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hyhdWxi4okEN"
   },
   "source": [
    "# Task 1 (3 points + 2 bonus points for a well organized and easy to read plot + 1 bonus point for the short comment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wv6bM-m_ow8p"
   },
   "source": [
    "Consider the following toy dataset with pairs of correlated features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yN5gx_1qonAl"
   },
   "outputs": [],
   "source": [
    "def gen_data(N, correlation=0.5):\n",
    "    # start from 4 independent features:\n",
    "    X = np.random.uniform(-1., 1., size=(N, 4))\n",
    "\n",
    "    y = X @ [2.3, -4.7, -1.7, 3.2] # true dependence\n",
    "\n",
    "    # add correlations, so that X[:,0] correlates with X[:,2] and\n",
    "    # X[:,1] correlates with X[:,3]:\n",
    "    X[:,2:] = correlation * X[:,:2] + (1 - correlation) * X[:,2:]\n",
    "\n",
    "    # add some noise to the targets\n",
    "    y += np.random.normal(size=y.shape)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "egoowPhmuWBW"
   },
   "source": [
    "Fit this dataset with Ridge and Lasso (**without the bias term!**), for different correlation levels and various regularization powers.\n",
    "\n",
    "Separately for Ridge and Lasso, and separately for different correlation levels, plot the fitted parameter values as a function of regularization power (x-axis in log scale). Make sure to make some color and pattern coding to make it clear which parameters correspond to correlating groups of features, e.g.:\n",
    "```python\n",
    "# Same color (red), different patterns (solid vs dashed) for features 0 and 2\n",
    "plt.plot(alpha_values, param_0, color='red', label='0')\n",
    "plt.plot(alpha_values, param_2, '--', color='red', label='2')\n",
    "\n",
    "# Same color (blue), different patterns (solid vs dashed) for features 1 and 3\n",
    "plt.plot(alpha_values, param_1, color='blue', label='1')\n",
    "plt.plot(alpha_values, param_3, '--', color='blue', label='3')\n",
    "```\n",
    "Please also add legends, axis labels and/or titles to be able to understand what's shown on this or that plot.\n",
    "\n",
    "What do you observe? How regularization affects the two kinds of models for different correlation levels? Write a short comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5k6TNTssqvHs"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for correlation in [0.1, 0.5, 0.9]:\n",
    "    # dataset to fit (no need to split into train/test in this task):\n",
    "    X, y = gen_data(500, correlation)\n",
    "\n",
    "    # regularization powers:\n",
    "    alpha_values = np.logspace(-4, 4, 100, base=10)\n",
    "\n",
    "    # lists to collect models' parameters:\n",
    "    params_ridge = []\n",
    "    params_lasso = []\n",
    "\n",
    "    for alpha in tqdm(alpha_values):\n",
    "        ridge = Ridge(alpha=alpha)\n",
    "        ridge.fit(X, y)\n",
    "        params_ridge.append(ridge.coef_)\n",
    "        \n",
    "        lasso = Lasso(alpha=alpha)\n",
    "        lasso.fit(X, y)\n",
    "        params_lasso.append(lasso.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_versus_alpha_plot(weight, alphas, features, regularizer_name):\n",
    "    \"\"\"\n",
    "    Pass in the estimated weight, the alpha value and the names\n",
    "    for the features and plot the model's estimated coefficient weight \n",
    "    for different alpha values\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize = (8, 6))\n",
    "    \n",
    "    # ensure that the weight is an array\n",
    "    weight = np.array(weight)\n",
    "    for col in range(weight.shape[1]):\n",
    "        plt.plot(np.log(alphas), weight[:, col], label = features[col])\n",
    "\n",
    "    plt.axhline(0, color = 'black', linestyle = '--', linewidth = 3)\n",
    "    \n",
    "    # manually specify the coordinate of the legend\n",
    "    plt.legend(bbox_to_anchor = (1.3, 0.9))\n",
    "    plt.title(f'Coefficient Weight as Alpha Grows --{regularizer_name}')\n",
    "    plt.ylabel('Coefficient weight')\n",
    "    plt.xlabel('alpha')\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change default figure and font size\n",
    "plt.rcParams['figure.figsize'] = 16, 12\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "ridge_fig = weight_versus_alpha_plot(params_ridge, alpha_values, ['param_0', 'param_1', 'param_2', 'param_3'], 'Ridge Regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change default figure and font size\n",
    "plt.rcParams['figure.figsize'] = 16, 12\n",
    "plt.rcParams['font.size'] = 12\n",
    "lasso_fig = weight_versus_alpha_plot(params_lasso, alpha_values, ['param_0', 'param_1', 'param_2', 'param_3'], 'Lasso Regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the two graphs presented above, we can see that as the penalty value, alpha increases:\n",
    "\n",
    "Lasso regression shrinks coefficients to zero, thus removing them from the model.\n",
    "Ridge regression however shrinks coefficients almost to zero, but they rarely reach zero or go beyond zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMR6eWeOZ3GYaeShpM8bTDv",
   "include_colab_link": true,
   "name": "MLDM_2020_seminar04_homework.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
